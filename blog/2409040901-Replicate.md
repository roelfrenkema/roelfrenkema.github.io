# Replicate FLUX1 training

[Return to Blog](/blog) - [Return to Index](/)

I have been working to automate my worflow in training lora with the replicate API. I now work with a singli directory of images that will be fully automated captioned and send to replicate.

I will not go into the fine detailes. Give you some room to learn and adapt. But I will illustrate the steps needed and some example scripts.

## Step 1. Image collection.

Lets face it most people swear by Google. But did you ever take a look at Bing. It is really amzing in the way you can select messages there, and the choice, size and quality of the images.

You dont need more then 10 to 20 images for FLUX, but make sure that they are divers and show your subject well. As usually pervent repetitions.

Once you collected then you might want to resize them as Replicate dont like oversized trainings data. I use a small batch file file for that:

```
#!/usr/bin/env zsh

for file in *.png; do
  # do something with $file
   echo "Processing $file"
    convert $file -resize $1 $file
done
```

That finishes the first step.

## Step 2. Captioning.

I know there are plenty of programs helping to capion images. But did you know you dont need them? First of instead of keywords Flux is very happy with natural language. In the following Python script we produce that together with a trigger word.

The script uses a local model named **xtuner_llava-phi-3-mini-hf** You can download it on huggingface and change the line with YOURPATH in it to where you placed the model directory. Ofcause you can adapt the script to other models.

I call the script with two arguments, the first one the directory where your images are. The second one a token that you want to add to the description. It makes identifying your LoRa in your prompt easier.

```
#
# works with transformers-4.44.2
# but latest github transformers will break it
#
import argparse
import torch
import os
from PIL import Image
from transformers import BitsAndBytesConfig, pipeline, GenerationConfig, AutoConfig

# Define command line argument parser                                                                                                                                           
parser = argparse.ArgumentParser(description='get dir')                                                                                                                   
parser.add_argument('dir', type=str, help='dir name')                                                                              
parser.add_argument('tok', type=str, help='token name')                                                                              
args = parser.parse_args()                                                                                                                                                      
                                                                                                                                                                                
# Define quantization configuration
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

# Define the model ID directory
model_id = "/YOURPATH/xtuner_llava-phi-3-mini-hf"

# Define the generation configuration
#generation_config = GenerationConfig(max_new_tokens=100)

# Create the pipeline
pipe = pipeline("image-to-text", model=model_id, model_kwargs={"quantization_config": quantization_config, "low_cpu_mem_usage": True})

# Define the prompt
prompt = "<|user|>\n<image>\nGive a description of the image.<|end|>\n<|assistant|>\n"

# Loop through all files in the directory
for filename in os.listdir(args.dir):

    # Check if the file is a PNG image
    if filename.endswith(".png"):
        # Open the image
        image = Image.open(f"{args.dir}/{filename}")
        print(f"Image: {filename}")

        # Generate text
        outputs = pipe(image, prompt=prompt, generate_kwargs={"max_new_tokens": 100})

        # Get rid of empty lines
        generated_text = outputs[0]['generated_text'].replace("Give a description of the image. ", f"{args.tok}, ")
        lines = generated_text.splitlines()
        non_empty_lines = [line for line in lines if line.strip()]
        result = ' '.join(non_empty_lines)

        #print(f"{result}\n")

        # Save the result to a file
        with open(os.path.splitext(f"{args.dir}/{filename}")[0] + ".txt", "w") as f:
            f.write(result)

```

All straight forward as you can see. After running the script you image directory should have a .txt file for each image. With tags we needed to edit them and see if everything was fitting the image. With the natural language approach this is not realy needed.

Now you can just zip up all to a zipfile with the name source.zip which should be in the image directory for the next step.

## Step 3. Training.

We are now all set to invoke training on Replicate. The folowing takes care of that with 3 arguments, the image directory, the token and a description. As stuff goes wrong sometimes  after model creation and breaks in the Replicate example because the model exsists I have build a check for that.

To run it you should make sure your Replicate api key is in the enveronment. And while writing I think you should get the Huggingface key from there to if you need it. So I will change the script later. 

Replace owner with your user name. Also if you use Hug do not forget to create a model there before starting to train.

```
import replicate
import argparse
from replicate import models
from replicate.exceptions import ReplicateError

# Define command line argument parser
parser = argparse.ArgumentParser(description='Train a model')
parser.add_argument('name', type=str, help='The name of the global variable for the model')
parser.add_argument('token', type=str, help='The name of the token')
parser.add_argument('description', type=str, help='The description')
args = parser.parse_args()

# Assign command line arguments to global variables
owner = "YOURNAME"

try:
    # Try to create a new model
    model = models.create(
        name=f"{args.name}",
        owner=f"{owner}",
        visibility="public",  # or "private" if you prefer
        hardware="gpu-t4",  # Replicate will override this for fine-tuned models
        description=f"{args.description}"
    )
    print(f"Model created: {args.name}")
except ReplicateError as e:
    if f'A model with that name and owner already exists.' in str(e):
        print("Model already exists, using it for training.")
    else:
        raise  # If this is not a "model already exists" error, re-raise the exception.
	
# Now use this model as the destination for your training

training = replicate.trainings.create(
    version="ostris/flux-dev-lora-trainer:7f53f82066bcdfb1c549245a624019c26ca6e3c8034235cd4826425b61e77bec",
    input={
        "input_images": open(f"{args.name}/source.zip", "rb"),
        "steps": 1000,
        "lora_rank": 16,
        "optimizer": "adamw8bit",
        "batch_size": 1,
        "resolution": "512,768,1024",
        "autocaption": False,
        "trigger_word": f"{args.token}",
        "learning_rate": 0.0004,
        "hf_token": "hf_XXXXXXXXXXXXXXXXXXX",  # optional
        "hf_repo_id": f"{owner}/{args.name}",  # optional
    },
    destination=f"{owner}/{args.name}"
)

print(f"Training started: {training.status}")
print(f"Training URL: https://replicate.com/p/{training.id}")
```

## About this approach.

As you can see the steps are easy to follow. I combine them all in a single batch file. So once I collected the images I call it and it runs. Simple. It could be improved by training yourself. At this moment my rtx3060 is not up to is so no use to me.

At some point the caption script will complain
```
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset                                                  
```
I need to dig into that and learn a little more. I daubt though it makes much sense if you only use a few images like her. You can safely ignore the comment though.

I hope this scripts are helpfull and can teach you some stuff. You can contact me on Discord @geennaam or Huggingface https://huggingface.co/roelfrenkema if you have any questions. On Discord I am most active in the [Straic.com](straico.com) channels.

[Return to Blog](/blog) - [Return to Index](/)
